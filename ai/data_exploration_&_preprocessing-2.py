# -*- coding: utf-8 -*-
"""Data Exploration & Preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pe8e_TeFiM0CtK5T_QTCpIVj3D8vFckw
"""

import pandas as pd

logs = pd.read_csv("activity_logs.csv")
quizzes = pd.read_csv("quiz_attempts.csv")
assignments = pd.read_csv("assignment_submissions.csv")
results = pd.read_csv("course_results.csv")

"""Engagement Features"""

engagement = logs.groupby(["student_id", "course_id"]).agg(
    total_time_spent=("time_spent_sec", "sum"),
    total_events=("event_type", "count"),
    avg_time_per_event=("time_spent_sec", "mean"),
    active_days=("timestamp", lambda x: x.nunique())
).reset_index()

engagement["access_frequency"] = engagement["total_events"] / engagement["active_days"]

"""Quiz Features"""

quiz_features = quizzes.groupby(
    ["student_id", quizzes["activity_id"].str[:3]]
).agg(
    avg_quiz_score=("score", "mean"),
    quiz_attempt_count=("attempt_id", "count"),
    quiz_score_std=("score", "std"),
    avg_quiz_time=("time_spent_sec", "mean")
).reset_index().rename(columns={"activity_id": "course_id"})

"""Assignement Features"""

assignment_features = assignments.groupby(
    ["student_id", assignments["activity_id"].str[:3]]
).agg(
    avg_assignment_score=("score", "mean"),
    late_submission_ratio=("late_submission", "mean"),
    assignment_count=("submission_id", "count")
).reset_index().rename(columns={"activity_id": "course_id"})

"""Merge everything & Fill missing values"""

features = engagement \
    .merge(quiz_features, on=["student_id", "course_id"], how="left") \
    .merge(assignment_features, on=["student_id", "course_id"], how="left") \
    .merge(results, on=["student_id", "course_id"], how="left")

features.fillna(0, inplace=True)

"""Select Clustering Features

"""

clustering_features = features[
    [
"total_time_spent",
"total_events",
"active_days",
"avg_time_per_event",
"access_frequency"
    ]
]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(clustering_features)

"""Behaviour Flags"""

features["low_engagement_flag"] = (features["total_time_spent"] < 3000).astype(int)
features["high_time_low_score_flag"] = (
    (features["total_time_spent"] > 10000) &
    (features["avg_quiz_score"] < 5)
).astype(int)

features.to_csv("student_course_features.csv", index=False)

"""Fixing Data"""

risk_records = []

for _, row in suspects.iterrows():
    risk_records.append({
        "student_id": row["student_id"] + "_RISK",
        "total_time_spent": row["total_time_spent"] * random.uniform(0.4, 0.6),
        "total_events": row["total_events"] * random.uniform(0.4, 0.6),
        "avg_time_per_event": row["avg_time_per_event"],
        "active_days": max(5, row["active_days"] * 0.5),
        "access_frequency": row["access_frequency"] * 0.6,
        "avg_quiz_score": random.uniform(2.5, 4.5),
        "quiz_attempt_count": random.randint(1, 5),
        "quiz_score_std": random.uniform(2.5, 4),
        "avg_quiz_time": row["avg_quiz_time"],
        "avg_assignment_score": random.uniform(3, 5),
        "late_submission_ratio": random.uniform(0.5, 0.9),
        "assignment_count": random.randint(1, 2),
        "avg_final_grade": random.uniform(3, 4.9),
        "failed_courses": random.randint(1, 3)
    })

risk_df = pd.DataFrame(risk_records)

features_expanded = pd.concat(
    [features, risk_df],
    ignore_index=True
)

risk_df = pd.DataFrame(risk_records)

features_expanded = pd.concat(
    [features, risk_df],
    ignore_index=True
)

"""reselect cluster features"""

clustering_features = features_expanded[
    [
        "total_time_spent",
        "access_frequency",
        "avg_quiz_score",
        "avg_assignment_score",
        "late_submission_ratio",
        "failed_courses",
        "active_days"
    ]
]

from sklearn.preprocessing import StandardScaler

X_scaled = StandardScaler().fit_transform(clustering_features)

"""Apply K-means cluster (3)"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
features_expanded["risk_cluster"] = kmeans.fit_predict(X_scaled)

cluster_summary = (
    features
    .groupby("risk_cluster")
    .mean(numeric_only=True)
)

# 1. Verify clustering worked
features["risk_cluster"].value_counts()

# 2. FIXED cluster summary
cluster_summary = features.groupby("risk_cluster").mean(numeric_only=True)
cluster_summary

